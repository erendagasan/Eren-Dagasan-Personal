{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erendagasan/Eren-Dagasan-Personal/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Libraries and Indicator Function\n",
        "\n",
        "!pip install -q bta-lib\n",
        "!pip install -q ta\n",
        "\n",
        "import btalib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ta.trend import PSARIndicator\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.trend import AroonIndicator\n",
        "from ta.volume import VolumePriceTrendIndicator\n",
        "from ta.trend import CCIIndicator\n",
        "from ta.momentum import ROCIndicator\n",
        "from ta.trend import ADXIndicator\n",
        "from ta.momentum import ultimate_oscillator\n",
        "from ta.volume import ChaikinMoneyFlowIndicator\n",
        "from ta.trend import KSTIndicator\n",
        "from ta.momentum import TSIIndicator\n",
        "from ta.trend import WMAIndicator\n",
        "import yfinance as yf\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def create_indicators(data):\n",
        "  data[\"RSI-14\"] = btalib.rsi(data[\"Close\"], period=14).df\n",
        "  data[\"RSI-60\"] = btalib.rsi(data[\"Close\"], period=60).df\n",
        "\n",
        "  data[\"STOCH-K\"] = btalib.stoch(data['High'], data['Low'], data['Close']).df[\"k\"]\n",
        "  data[\"STOCH-D\"] = btalib.stoch(data['High'], data['Low'], data['Close']).df[\"d\"]\n",
        "\n",
        "  data[\"WILLIAMS\"] = WilliamsRIndicator(data[\"High\"], data[\"Low\"], data[\"Close\"]).williams_r()\n",
        "  data[\"AROON\"] = AroonIndicator(close=data[\"Close\"], window=25).aroon_indicator()\n",
        "  data['CCI'] = CCIIndicator(close=data['Close'], low=data[\"Low\"], high=data[\"High\"], window=14).cci()\n",
        "  data['ROC'] = ROCIndicator(close=data['Close'], window=5).roc()\n",
        "\n",
        "  adx_indicator = ADXIndicator(high=data['High'], low=data['Low'], close=data['Close'], window=14)\n",
        "  data['ADX'] = adx_indicator.adx()\n",
        "  data['+DI'] = adx_indicator.adx_pos()\n",
        "  data['-DI'] = adx_indicator.adx_neg()\n",
        "\n",
        "  data['ULTIMATE-OSC'] = ultimate_oscillator(high=data['High'], low=data['Low'], close=data['Close'], window1=7, window2=14, window3=28)\n",
        "  data['MONEY-FLOW'] = ChaikinMoneyFlowIndicator(high=data['High'], low=data['Low'], close=data['Close'], volume=data['Volume'], window=20).chaikin_money_flow()\n",
        "  data['KST'] = KSTIndicator(data['Close']).kst()\n",
        "\n",
        "  data['TSI'] = TSIIndicator(data['Close']).tsi()\n",
        "\n",
        "  data['WMA-30'] = WMAIndicator(data['Close'], window=30).wma()\n",
        "\n",
        "  data = data.dropna()\n",
        "  data = data.reset_index()\n",
        "  return data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Lfv1WZ9SIIPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59c70ed-048b-4d60-be37-668fa6f5425e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the Model\n",
        "\n",
        "import gdown\n",
        "\n",
        "gdown.download(\"https://drive.google.com/u/1/uc?id=1-3r7tu0ZQXWNtqMQ35rX0DexxNt9P1l5&export=download\", \"/content/\", quiet=False)\n",
        "# gdown.download(\"https://drive.google.com/u/0/uc?id=117pezAA6jRLCwIsdpEhZgEa9tEanlC0O&export=download\", \"/content/\", quiet=False)\n",
        "\n",
        "# data = pd.read_csv(\"data.csv\")\n",
        "model = tf.keras.models.load_model(\"best_model_2.h5\")"
      ],
      "metadata": {
        "id": "j0_ac_MyLUax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stock List\n",
        "sheet_id = \"1RSqOXkFTAO7g4H9LEY3d3IX6H6bJaYk1\"\n",
        "sheet_name = \"Sheet_1\"\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "result_df = pd.read_csv(url)\n",
        "\n",
        "sheet_id = \"1AA9MfqOtAAgO97__aomD79DciyT-PkRQ\"\n",
        "sheet_name = \"Sheet_1\"\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "result_df = pd.read_csv(url)\n",
        "\n",
        "nasdaq100 = ['AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN',\n",
        "             'NVDA', 'TSLA', 'META', 'AVGO', 'ASML',\n",
        "             'PEP', 'COST', 'ADBE', 'AZN', 'CSCO',\n",
        "             'NFLX', 'AMD', 'CMCSA', 'TMUS', 'TXN',\n",
        "             'QCOM', 'HON', 'INTU', 'INTC', 'SNY',\n",
        "             'VZ', 'AMGN', 'SBUX', 'ISRG', 'AMAT',\n",
        "             'BKNG', 'ADI', 'MDLZ', 'PDD', 'GILD',\n",
        "             'ADP', 'VRTX', 'ABNB', 'LRCX', 'PYPL',\n",
        "             'REGN', 'EQIX', 'MU', 'CSX', 'SNPS',\n",
        "             'CME', 'CDNS', 'KLAC', 'NTES']"
      ],
      "metadata": {
        "id": "3g0KHl4YMTKd",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "for stock in [\"AAPL\"]:\n",
        "  stock_df = yf.download(stock, start=\"2010-01-01\", end=\"2023-01-01\", progress=False)\n",
        "  stock_df = create_indicators(stock_df)\n",
        "  stock_df[\"signal\"] = 0\n",
        "\n",
        "  for index, row in stock_df.iterrows():\n",
        "    if index > 0 and index < stock_df.shape[0]-1 and stock_df[\"Close\"].iloc[index+1] > ((1.5*stock_df[\"Close\"].iloc[index]/100) + stock_df[\"Close\"].iloc[index]):\n",
        "      stock_df[\"signal\"].iloc[index] = 1\n",
        "\n",
        "  stock_df = stock_df.drop([\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"], axis=1)\n",
        "\n",
        "  data = pd.concat([data, stock_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "301D-_cbzUbS"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Preprocessing\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "input_columns = df.columns[:data.shape[1]-1]\n",
        "output_column = \"signal\"\n",
        "\n",
        "df[output_column] = df[output_column].astype(int)\n",
        "\n",
        "X = df[input_columns].values\n",
        "y = df[output_column].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(\"balanced\", classes=[0, 1], y=y_train)\n",
        "class_weight = {cls: weight for cls, weight in zip([0, 1], class_weights)}\n",
        "class_weight"
      ],
      "metadata": {
        "id": "K-Gz1pgOMh8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e49fff6-774b-4cdd-b749-d2b4890d502b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.6061821613968853, 1: 2.8544444444444443}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LSTM Model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X_train_resampled.shape[1], 1)),\n",
        "\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    # tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    # tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    # tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "    # tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('lstm_model.h5', save_best_only=True)\n",
        "\n",
        "model.fit(X_train_resampled, y_train_resampled, epochs=1000, batch_size=16, validation_split=0.2, class_weight=class_weight,\n",
        "          callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "IfSHE91eZonR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80d6346-26a7-4e89-f0a0-4ba063e49db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "212/212 [==============================] - 6s 13ms/step - loss: 0.8383 - accuracy: 0.3776 - val_loss: 0.3437 - val_accuracy: 1.0000\n",
            "Epoch 2/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.8220 - accuracy: 0.3749 - val_loss: 0.3210 - val_accuracy: 1.0000\n",
            "Epoch 3/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.8200 - accuracy: 0.3749 - val_loss: 0.3107 - val_accuracy: 1.0000\n",
            "Epoch 4/1000\n",
            "212/212 [==============================] - 2s 7ms/step - loss: 0.8172 - accuracy: 0.3770 - val_loss: 0.3558 - val_accuracy: 1.0000\n",
            "Epoch 5/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.8196 - accuracy: 0.3749 - val_loss: 0.3314 - val_accuracy: 1.0000\n",
            "Epoch 6/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.8174 - accuracy: 0.3785 - val_loss: 0.2877 - val_accuracy: 1.0000\n",
            "Epoch 7/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.8137 - accuracy: 0.3749 - val_loss: 0.3365 - val_accuracy: 1.0000\n",
            "Epoch 8/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.8106 - accuracy: 0.3752 - val_loss: 0.2823 - val_accuracy: 0.9847\n",
            "Epoch 9/1000\n",
            "212/212 [==============================] - 2s 12ms/step - loss: 0.8102 - accuracy: 0.3791 - val_loss: 0.2853 - val_accuracy: 1.0000\n",
            "Epoch 10/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.8073 - accuracy: 0.3850 - val_loss: 0.3133 - val_accuracy: 1.0000\n",
            "Epoch 11/1000\n",
            "212/212 [==============================] - 2s 7ms/step - loss: 0.8042 - accuracy: 0.3965 - val_loss: 0.2939 - val_accuracy: 1.0000\n",
            "Epoch 12/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.8026 - accuracy: 0.3950 - val_loss: 0.2928 - val_accuracy: 0.9976\n",
            "Epoch 13/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.7977 - accuracy: 0.4177 - val_loss: 0.2703 - val_accuracy: 0.9906\n",
            "Epoch 14/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.7992 - accuracy: 0.4086 - val_loss: 0.2801 - val_accuracy: 0.9941\n",
            "Epoch 15/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.7904 - accuracy: 0.4053 - val_loss: 0.2897 - val_accuracy: 0.9658\n",
            "Epoch 16/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.7833 - accuracy: 0.4351 - val_loss: 0.2625 - val_accuracy: 0.9776\n",
            "Epoch 17/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.7716 - accuracy: 0.4496 - val_loss: 0.2792 - val_accuracy: 0.9729\n",
            "Epoch 18/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.7486 - accuracy: 0.4779 - val_loss: 0.3285 - val_accuracy: 0.9316\n",
            "Epoch 19/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.7613 - accuracy: 0.4808 - val_loss: 0.3052 - val_accuracy: 0.9776\n",
            "Epoch 20/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.7360 - accuracy: 0.4799 - val_loss: 0.2570 - val_accuracy: 0.9493\n",
            "Epoch 21/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.7230 - accuracy: 0.5053 - val_loss: 0.2556 - val_accuracy: 0.9575\n",
            "Epoch 22/1000\n",
            "212/212 [==============================] - 2s 7ms/step - loss: 0.7033 - accuracy: 0.5366 - val_loss: 0.2671 - val_accuracy: 0.9410\n",
            "Epoch 23/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.6648 - accuracy: 0.5684 - val_loss: 0.2180 - val_accuracy: 0.9776\n",
            "Epoch 24/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.6509 - accuracy: 0.5773 - val_loss: 0.2239 - val_accuracy: 0.9658\n",
            "Epoch 25/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.6218 - accuracy: 0.6065 - val_loss: 0.2147 - val_accuracy: 0.9693\n",
            "Epoch 26/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.6120 - accuracy: 0.6150 - val_loss: 0.2335 - val_accuracy: 0.9481\n",
            "Epoch 27/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.5825 - accuracy: 0.6392 - val_loss: 0.2134 - val_accuracy: 0.9611\n",
            "Epoch 28/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.5467 - accuracy: 0.6735 - val_loss: 0.1849 - val_accuracy: 0.9705\n",
            "Epoch 29/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.5310 - accuracy: 0.6853 - val_loss: 0.1980 - val_accuracy: 0.9587\n",
            "Epoch 30/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.5105 - accuracy: 0.7009 - val_loss: 0.1857 - val_accuracy: 0.9623\n",
            "Epoch 31/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.4739 - accuracy: 0.7236 - val_loss: 0.1919 - val_accuracy: 0.9505\n",
            "Epoch 32/1000\n",
            "212/212 [==============================] - 3s 12ms/step - loss: 0.4340 - accuracy: 0.7537 - val_loss: 0.1417 - val_accuracy: 0.9741\n",
            "Epoch 33/1000\n",
            "212/212 [==============================] - 3s 14ms/step - loss: 0.4119 - accuracy: 0.7605 - val_loss: 0.1416 - val_accuracy: 0.9682\n",
            "Epoch 34/1000\n",
            "212/212 [==============================] - 4s 17ms/step - loss: 0.3677 - accuracy: 0.7985 - val_loss: 0.1087 - val_accuracy: 0.9917\n",
            "Epoch 35/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.3430 - accuracy: 0.8209 - val_loss: 0.0988 - val_accuracy: 0.9776\n",
            "Epoch 36/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.3794 - accuracy: 0.7962 - val_loss: 0.1207 - val_accuracy: 0.9634\n",
            "Epoch 37/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.3100 - accuracy: 0.8445 - val_loss: 0.1132 - val_accuracy: 0.9800\n",
            "Epoch 38/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.3075 - accuracy: 0.8454 - val_loss: 0.1364 - val_accuracy: 0.9540\n",
            "Epoch 39/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.2876 - accuracy: 0.8525 - val_loss: 0.1165 - val_accuracy: 0.9693\n",
            "Epoch 40/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.2887 - accuracy: 0.8599 - val_loss: 0.0905 - val_accuracy: 0.9929\n",
            "Epoch 41/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.2518 - accuracy: 0.8735 - val_loss: 0.0831 - val_accuracy: 0.9858\n",
            "Epoch 42/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.2010 - accuracy: 0.9032 - val_loss: 0.0643 - val_accuracy: 0.9894\n",
            "Epoch 43/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1995 - accuracy: 0.9032 - val_loss: 0.0979 - val_accuracy: 0.9882\n",
            "Epoch 44/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.1846 - accuracy: 0.9103 - val_loss: 0.0890 - val_accuracy: 0.9717\n",
            "Epoch 45/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.2559 - accuracy: 0.8779 - val_loss: 0.0800 - val_accuracy: 0.9847\n",
            "Epoch 46/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.1599 - accuracy: 0.9277 - val_loss: 0.0464 - val_accuracy: 0.9906\n",
            "Epoch 47/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1438 - accuracy: 0.9366 - val_loss: 0.0329 - val_accuracy: 0.9965\n",
            "Epoch 48/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.1089 - accuracy: 0.9522 - val_loss: 0.0706 - val_accuracy: 0.9800\n",
            "Epoch 49/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.2873 - accuracy: 0.8723 - val_loss: 0.0901 - val_accuracy: 0.9764\n",
            "Epoch 50/1000\n",
            "212/212 [==============================] - 2s 7ms/step - loss: 0.1707 - accuracy: 0.9221 - val_loss: 0.0426 - val_accuracy: 0.9953\n",
            "Epoch 51/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0952 - accuracy: 0.9611 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
            "Epoch 52/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.1070 - accuracy: 0.9543 - val_loss: 0.0565 - val_accuracy: 0.9823\n",
            "Epoch 53/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.1857 - accuracy: 0.9268 - val_loss: 0.0493 - val_accuracy: 0.9882\n",
            "Epoch 54/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1218 - accuracy: 0.9469 - val_loss: 0.0335 - val_accuracy: 0.9965\n",
            "Epoch 55/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0793 - accuracy: 0.9649 - val_loss: 0.0440 - val_accuracy: 0.9835\n",
            "Epoch 56/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.2158 - accuracy: 0.9127 - val_loss: 0.0604 - val_accuracy: 0.9870\n",
            "Epoch 57/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0966 - accuracy: 0.9569 - val_loss: 0.0190 - val_accuracy: 0.9953\n",
            "Epoch 58/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0593 - accuracy: 0.9767 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 59/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0383 - accuracy: 0.9853 - val_loss: 0.0242 - val_accuracy: 0.9953\n",
            "Epoch 60/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.2839 - accuracy: 0.8923 - val_loss: 0.1989 - val_accuracy: 0.9481\n",
            "Epoch 61/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.1722 - accuracy: 0.9186 - val_loss: 0.0440 - val_accuracy: 0.9953\n",
            "Epoch 62/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0595 - accuracy: 0.9758 - val_loss: 0.0354 - val_accuracy: 0.9882\n",
            "Epoch 63/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0627 - accuracy: 0.9755 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 64/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0323 - accuracy: 0.9891 - val_loss: 0.0222 - val_accuracy: 0.9929\n",
            "Epoch 65/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0585 - accuracy: 0.9791 - val_loss: 0.0198 - val_accuracy: 0.9965\n",
            "Epoch 66/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0557 - accuracy: 0.9814 - val_loss: 0.0154 - val_accuracy: 0.9941\n",
            "Epoch 67/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.1484 - accuracy: 0.9481 - val_loss: 0.2260 - val_accuracy: 0.9245\n",
            "Epoch 68/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.2220 - accuracy: 0.9059 - val_loss: 0.0434 - val_accuracy: 0.9929\n",
            "Epoch 69/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0817 - accuracy: 0.9658 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 70/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0293 - accuracy: 0.9894 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 71/1000\n",
            "212/212 [==============================] - 2s 7ms/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 72/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1402 - accuracy: 0.9519 - val_loss: 0.0494 - val_accuracy: 0.9906\n",
            "Epoch 73/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0901 - accuracy: 0.9628 - val_loss: 0.0252 - val_accuracy: 0.9917\n",
            "Epoch 74/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0519 - accuracy: 0.9796 - val_loss: 0.0447 - val_accuracy: 0.9776\n",
            "Epoch 75/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.0470 - accuracy: 0.9844 - val_loss: 0.0434 - val_accuracy: 0.9800\n",
            "Epoch 76/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.0398 - accuracy: 0.9844 - val_loss: 0.0135 - val_accuracy: 0.9988\n",
            "Epoch 77/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0763 - accuracy: 0.9788 - val_loss: 0.0381 - val_accuracy: 0.9858\n",
            "Epoch 78/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1388 - accuracy: 0.9499 - val_loss: 0.0825 - val_accuracy: 0.9599\n",
            "Epoch 79/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1148 - accuracy: 0.9534 - val_loss: 0.0238 - val_accuracy: 0.9941\n",
            "Epoch 80/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0354 - accuracy: 0.9870 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 81/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 82/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.0084 - accuracy: 0.9979 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 83/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 84/1000\n",
            "212/212 [==============================] - 2s 7ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.0092 - val_accuracy: 0.9976\n",
            "Epoch 85/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1989 - accuracy: 0.9333 - val_loss: 0.0781 - val_accuracy: 0.9741\n",
            "Epoch 86/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1064 - accuracy: 0.9552 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 87/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9894 - val_loss: 0.0348 - val_accuracy: 0.9941\n",
            "Epoch 88/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0263 - accuracy: 0.9932 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 89/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.0105 - val_accuracy: 0.9965\n",
            "Epoch 90/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.0930 - accuracy: 0.9667 - val_loss: 0.0399 - val_accuracy: 0.9835\n",
            "Epoch 91/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.1981 - accuracy: 0.9265 - val_loss: 0.1269 - val_accuracy: 0.9623\n",
            "Epoch 92/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.1547 - accuracy: 0.9357 - val_loss: 0.0330 - val_accuracy: 0.9894\n",
            "Epoch 93/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.0554 - accuracy: 0.9773 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 94/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0209 - accuracy: 0.9935 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
            "Epoch 95/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0108 - accuracy: 0.9976 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 96/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 97/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.0050 - accuracy: 0.9991 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 98/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 99/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 100/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 101/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0600 - accuracy: 0.9814 - val_loss: 0.0517 - val_accuracy: 0.9858\n",
            "Epoch 102/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.2128 - accuracy: 0.9215 - val_loss: 0.0774 - val_accuracy: 0.9693\n",
            "Epoch 103/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0702 - accuracy: 0.9714 - val_loss: 0.0169 - val_accuracy: 0.9988\n",
            "Epoch 104/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.0431 - accuracy: 0.9844 - val_loss: 0.0245 - val_accuracy: 0.9929\n",
            "Epoch 105/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.0433 - accuracy: 0.9841 - val_loss: 0.0181 - val_accuracy: 0.9976\n",
            "Epoch 106/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0249 - accuracy: 0.9917 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 107/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0074 - accuracy: 0.9973 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 108/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0360 - accuracy: 0.9876 - val_loss: 0.0102 - val_accuracy: 0.9988\n",
            "Epoch 109/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0593 - accuracy: 0.9805 - val_loss: 0.0221 - val_accuracy: 0.9929\n",
            "Epoch 110/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.1013 - accuracy: 0.9661 - val_loss: 0.0486 - val_accuracy: 0.9811\n",
            "Epoch 111/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0682 - accuracy: 0.9749 - val_loss: 0.0462 - val_accuracy: 0.9894\n",
            "Epoch 112/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.0625 - accuracy: 0.9794 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 113/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.0120 - accuracy: 0.9968 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 114/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 115/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 116/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 8.5244e-04 - val_accuracy: 1.0000\n",
            "Epoch 117/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "Epoch 118/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0395 - accuracy: 0.9923 - val_loss: 0.1334 - val_accuracy: 0.9564\n",
            "Epoch 119/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 0.2545 - accuracy: 0.9118 - val_loss: 0.0595 - val_accuracy: 0.9800\n",
            "Epoch 120/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.0533 - accuracy: 0.9808 - val_loss: 0.0206 - val_accuracy: 0.9894\n",
            "Epoch 121/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0151 - accuracy: 0.9962 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 122/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0061 - accuracy: 0.9991 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 123/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 124/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 125/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 9.4598e-04 - val_accuracy: 1.0000\n",
            "Epoch 126/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 7.9448e-04 - val_accuracy: 1.0000\n",
            "Epoch 127/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 6.3733e-04 - val_accuracy: 1.0000\n",
            "Epoch 128/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 6.0633e-04 - val_accuracy: 1.0000\n",
            "Epoch 129/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.4744e-04 - val_accuracy: 1.0000\n",
            "Epoch 130/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 8.7339e-04 - accuracy: 1.0000 - val_loss: 3.9201e-04 - val_accuracy: 1.0000\n",
            "Epoch 131/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 7.4095e-04 - accuracy: 1.0000 - val_loss: 3.2836e-04 - val_accuracy: 1.0000\n",
            "Epoch 132/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 6.3724e-04 - accuracy: 1.0000 - val_loss: 3.0414e-04 - val_accuracy: 1.0000\n",
            "Epoch 133/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 5.5361e-04 - accuracy: 1.0000 - val_loss: 2.5064e-04 - val_accuracy: 1.0000\n",
            "Epoch 134/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 4.8071e-04 - accuracy: 1.0000 - val_loss: 2.3054e-04 - val_accuracy: 1.0000\n",
            "Epoch 135/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 4.1761e-04 - accuracy: 1.0000 - val_loss: 1.9258e-04 - val_accuracy: 1.0000\n",
            "Epoch 136/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.6831e-04 - accuracy: 1.0000 - val_loss: 1.7020e-04 - val_accuracy: 1.0000\n",
            "Epoch 137/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 3.2151e-04 - accuracy: 1.0000 - val_loss: 1.4834e-04 - val_accuracy: 1.0000\n",
            "Epoch 138/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 2.8405e-04 - accuracy: 1.0000 - val_loss: 1.2754e-04 - val_accuracy: 1.0000\n",
            "Epoch 139/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 2.5107e-04 - accuracy: 1.0000 - val_loss: 1.1236e-04 - val_accuracy: 1.0000\n",
            "Epoch 140/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 2.2056e-04 - accuracy: 1.0000 - val_loss: 9.8338e-05 - val_accuracy: 1.0000\n",
            "Epoch 141/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.9472e-04 - accuracy: 1.0000 - val_loss: 8.8341e-05 - val_accuracy: 1.0000\n",
            "Epoch 142/1000\n",
            "212/212 [==============================] - 2s 12ms/step - loss: 1.7310e-04 - accuracy: 1.0000 - val_loss: 7.7849e-05 - val_accuracy: 1.0000\n",
            "Epoch 143/1000\n",
            "212/212 [==============================] - 2s 12ms/step - loss: 1.5296e-04 - accuracy: 1.0000 - val_loss: 6.8383e-05 - val_accuracy: 1.0000\n",
            "Epoch 144/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.3553e-04 - accuracy: 1.0000 - val_loss: 6.0498e-05 - val_accuracy: 1.0000\n",
            "Epoch 145/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.1955e-04 - accuracy: 1.0000 - val_loss: 5.3181e-05 - val_accuracy: 1.0000\n",
            "Epoch 146/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.0655e-04 - accuracy: 1.0000 - val_loss: 4.7498e-05 - val_accuracy: 1.0000\n",
            "Epoch 147/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 9.4854e-05 - accuracy: 1.0000 - val_loss: 4.2244e-05 - val_accuracy: 1.0000\n",
            "Epoch 148/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 8.3882e-05 - accuracy: 1.0000 - val_loss: 3.7816e-05 - val_accuracy: 1.0000\n",
            "Epoch 149/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 7.4931e-05 - accuracy: 1.0000 - val_loss: 3.3239e-05 - val_accuracy: 1.0000\n",
            "Epoch 150/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 6.6344e-05 - accuracy: 1.0000 - val_loss: 3.0140e-05 - val_accuracy: 1.0000\n",
            "Epoch 151/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 5.9078e-05 - accuracy: 1.0000 - val_loss: 2.6670e-05 - val_accuracy: 1.0000\n",
            "Epoch 152/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 5.2622e-05 - accuracy: 1.0000 - val_loss: 2.3598e-05 - val_accuracy: 1.0000\n",
            "Epoch 153/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 4.6688e-05 - accuracy: 1.0000 - val_loss: 2.0951e-05 - val_accuracy: 1.0000\n",
            "Epoch 154/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 4.1467e-05 - accuracy: 1.0000 - val_loss: 1.8781e-05 - val_accuracy: 1.0000\n",
            "Epoch 155/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.6881e-05 - accuracy: 1.0000 - val_loss: 1.6509e-05 - val_accuracy: 1.0000\n",
            "Epoch 156/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 3.3037e-05 - accuracy: 1.0000 - val_loss: 1.4793e-05 - val_accuracy: 1.0000\n",
            "Epoch 157/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 2.9206e-05 - accuracy: 1.0000 - val_loss: 1.2782e-05 - val_accuracy: 1.0000\n",
            "Epoch 158/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.6062e-05 - accuracy: 1.0000 - val_loss: 1.1401e-05 - val_accuracy: 1.0000\n",
            "Epoch 159/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.3166e-05 - accuracy: 1.0000 - val_loss: 1.0293e-05 - val_accuracy: 1.0000\n",
            "Epoch 160/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.0603e-05 - accuracy: 1.0000 - val_loss: 9.0082e-06 - val_accuracy: 1.0000\n",
            "Epoch 161/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.8322e-05 - accuracy: 1.0000 - val_loss: 7.9486e-06 - val_accuracy: 1.0000\n",
            "Epoch 162/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6407e-05 - accuracy: 1.0000 - val_loss: 7.2252e-06 - val_accuracy: 1.0000\n",
            "Epoch 163/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.4551e-05 - accuracy: 1.0000 - val_loss: 6.6267e-06 - val_accuracy: 1.0000\n",
            "Epoch 164/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.2960e-05 - accuracy: 1.0000 - val_loss: 5.6658e-06 - val_accuracy: 1.0000\n",
            "Epoch 165/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.1527e-05 - accuracy: 1.0000 - val_loss: 4.9174e-06 - val_accuracy: 1.0000\n",
            "Epoch 166/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.0282e-05 - accuracy: 1.0000 - val_loss: 4.5517e-06 - val_accuracy: 1.0000\n",
            "Epoch 167/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 9.1377e-06 - accuracy: 1.0000 - val_loss: 4.1157e-06 - val_accuracy: 1.0000\n",
            "Epoch 168/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 8.1376e-06 - accuracy: 1.0000 - val_loss: 3.7028e-06 - val_accuracy: 1.0000\n",
            "Epoch 169/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 7.2568e-06 - accuracy: 1.0000 - val_loss: 3.2014e-06 - val_accuracy: 1.0000\n",
            "Epoch 170/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 6.4569e-06 - accuracy: 1.0000 - val_loss: 2.7866e-06 - val_accuracy: 1.0000\n",
            "Epoch 171/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 5.7684e-06 - accuracy: 1.0000 - val_loss: 2.4942e-06 - val_accuracy: 1.0000\n",
            "Epoch 172/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 5.1326e-06 - accuracy: 1.0000 - val_loss: 2.2398e-06 - val_accuracy: 1.0000\n",
            "Epoch 173/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 4.5827e-06 - accuracy: 1.0000 - val_loss: 2.0259e-06 - val_accuracy: 1.0000\n",
            "Epoch 174/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 4.0862e-06 - accuracy: 1.0000 - val_loss: 1.7988e-06 - val_accuracy: 1.0000\n",
            "Epoch 175/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.6289e-06 - accuracy: 1.0000 - val_loss: 1.5946e-06 - val_accuracy: 1.0000\n",
            "Epoch 176/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 3.2367e-06 - accuracy: 1.0000 - val_loss: 1.4364e-06 - val_accuracy: 1.0000\n",
            "Epoch 177/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.8825e-06 - accuracy: 1.0000 - val_loss: 1.2358e-06 - val_accuracy: 1.0000\n",
            "Epoch 178/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 2.5770e-06 - accuracy: 1.0000 - val_loss: 1.1178e-06 - val_accuracy: 1.0000\n",
            "Epoch 179/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 2.2930e-06 - accuracy: 1.0000 - val_loss: 9.7272e-07 - val_accuracy: 1.0000\n",
            "Epoch 180/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.0504e-06 - accuracy: 1.0000 - val_loss: 8.9583e-07 - val_accuracy: 1.0000\n",
            "Epoch 181/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.8229e-06 - accuracy: 1.0000 - val_loss: 7.8986e-07 - val_accuracy: 1.0000\n",
            "Epoch 182/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6302e-06 - accuracy: 1.0000 - val_loss: 7.0908e-07 - val_accuracy: 1.0000\n",
            "Epoch 183/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.4549e-06 - accuracy: 1.0000 - val_loss: 6.1483e-07 - val_accuracy: 1.0000\n",
            "Epoch 184/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.2987e-06 - accuracy: 1.0000 - val_loss: 5.5387e-07 - val_accuracy: 1.0000\n",
            "Epoch 185/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.1582e-06 - accuracy: 1.0000 - val_loss: 4.9222e-07 - val_accuracy: 1.0000\n",
            "Epoch 186/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.0346e-06 - accuracy: 1.0000 - val_loss: 4.3574e-07 - val_accuracy: 1.0000\n",
            "Epoch 187/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 9.2375e-07 - accuracy: 1.0000 - val_loss: 4.0490e-07 - val_accuracy: 1.0000\n",
            "Epoch 188/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 8.2706e-07 - accuracy: 1.0000 - val_loss: 3.5342e-07 - val_accuracy: 1.0000\n",
            "Epoch 189/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 7.3792e-07 - accuracy: 1.0000 - val_loss: 3.1066e-07 - val_accuracy: 1.0000\n",
            "Epoch 190/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 6.5844e-07 - accuracy: 1.0000 - val_loss: 2.7800e-07 - val_accuracy: 1.0000\n",
            "Epoch 191/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 5.8997e-07 - accuracy: 1.0000 - val_loss: 2.5385e-07 - val_accuracy: 1.0000\n",
            "Epoch 192/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 5.2792e-07 - accuracy: 1.0000 - val_loss: 2.2792e-07 - val_accuracy: 1.0000\n",
            "Epoch 193/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 4.7109e-07 - accuracy: 1.0000 - val_loss: 1.9984e-07 - val_accuracy: 1.0000\n",
            "Epoch 194/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 4.2236e-07 - accuracy: 1.0000 - val_loss: 1.8068e-07 - val_accuracy: 1.0000\n",
            "Epoch 195/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 3.7903e-07 - accuracy: 1.0000 - val_loss: 1.6487e-07 - val_accuracy: 1.0000\n",
            "Epoch 196/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.4040e-07 - accuracy: 1.0000 - val_loss: 1.4787e-07 - val_accuracy: 1.0000\n",
            "Epoch 197/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.0520e-07 - accuracy: 1.0000 - val_loss: 1.2914e-07 - val_accuracy: 1.0000\n",
            "Epoch 198/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.7403e-07 - accuracy: 1.0000 - val_loss: 1.1895e-07 - val_accuracy: 1.0000\n",
            "Epoch 199/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.4609e-07 - accuracy: 1.0000 - val_loss: 1.0772e-07 - val_accuracy: 1.0000\n",
            "Epoch 200/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.2141e-07 - accuracy: 1.0000 - val_loss: 9.7698e-08 - val_accuracy: 1.0000\n",
            "Epoch 201/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.9930e-07 - accuracy: 1.0000 - val_loss: 8.6632e-08 - val_accuracy: 1.0000\n",
            "Epoch 202/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.8004e-07 - accuracy: 1.0000 - val_loss: 7.8958e-08 - val_accuracy: 1.0000\n",
            "Epoch 203/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6288e-07 - accuracy: 1.0000 - val_loss: 7.2599e-08 - val_accuracy: 1.0000\n",
            "Epoch 204/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.4742e-07 - accuracy: 1.0000 - val_loss: 6.7455e-08 - val_accuracy: 1.0000\n",
            "Epoch 205/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.3359e-07 - accuracy: 1.0000 - val_loss: 6.1339e-08 - val_accuracy: 1.0000\n",
            "Epoch 206/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.2119e-07 - accuracy: 1.0000 - val_loss: 5.6315e-08 - val_accuracy: 1.0000\n",
            "Epoch 207/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.1056e-07 - accuracy: 1.0000 - val_loss: 5.1300e-08 - val_accuracy: 1.0000\n",
            "Epoch 208/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.0068e-07 - accuracy: 1.0000 - val_loss: 4.7986e-08 - val_accuracy: 1.0000\n",
            "Epoch 209/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 9.2224e-08 - accuracy: 1.0000 - val_loss: 4.4390e-08 - val_accuracy: 1.0000\n",
            "Epoch 210/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 8.4422e-08 - accuracy: 1.0000 - val_loss: 4.1991e-08 - val_accuracy: 1.0000\n",
            "Epoch 211/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 7.7905e-08 - accuracy: 1.0000 - val_loss: 3.8656e-08 - val_accuracy: 1.0000\n",
            "Epoch 212/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 7.1550e-08 - accuracy: 1.0000 - val_loss: 3.7195e-08 - val_accuracy: 1.0000\n",
            "Epoch 213/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 6.6348e-08 - accuracy: 1.0000 - val_loss: 3.4897e-08 - val_accuracy: 1.0000\n",
            "Epoch 214/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 6.1889e-08 - accuracy: 1.0000 - val_loss: 3.2851e-08 - val_accuracy: 1.0000\n",
            "Epoch 215/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 5.7682e-08 - accuracy: 1.0000 - val_loss: 3.0991e-08 - val_accuracy: 1.0000\n",
            "Epoch 216/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 5.3500e-08 - accuracy: 1.0000 - val_loss: 2.9579e-08 - val_accuracy: 1.0000\n",
            "Epoch 217/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 5.0126e-08 - accuracy: 1.0000 - val_loss: 2.8083e-08 - val_accuracy: 1.0000\n",
            "Epoch 218/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 4.7078e-08 - accuracy: 1.0000 - val_loss: 2.7146e-08 - val_accuracy: 1.0000\n",
            "Epoch 219/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 4.4350e-08 - accuracy: 1.0000 - val_loss: 2.5929e-08 - val_accuracy: 1.0000\n",
            "Epoch 220/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 4.2252e-08 - accuracy: 1.0000 - val_loss: 2.5463e-08 - val_accuracy: 1.0000\n",
            "Epoch 221/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 4.0185e-08 - accuracy: 1.0000 - val_loss: 2.4823e-08 - val_accuracy: 1.0000\n",
            "Epoch 222/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.8541e-08 - accuracy: 1.0000 - val_loss: 2.3337e-08 - val_accuracy: 1.0000\n",
            "Epoch 223/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 3.6414e-08 - accuracy: 1.0000 - val_loss: 2.3567e-08 - val_accuracy: 1.0000\n",
            "Epoch 224/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 3.5211e-08 - accuracy: 1.0000 - val_loss: 2.3245e-08 - val_accuracy: 1.0000\n",
            "Epoch 225/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.4101e-08 - accuracy: 1.0000 - val_loss: 2.2527e-08 - val_accuracy: 1.0000\n",
            "Epoch 226/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.2581e-08 - accuracy: 1.0000 - val_loss: 2.1807e-08 - val_accuracy: 1.0000\n",
            "Epoch 227/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 3.1348e-08 - accuracy: 1.0000 - val_loss: 2.1261e-08 - val_accuracy: 1.0000\n",
            "Epoch 228/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 3.0285e-08 - accuracy: 1.0000 - val_loss: 2.0879e-08 - val_accuracy: 1.0000\n",
            "Epoch 229/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 2.9392e-08 - accuracy: 1.0000 - val_loss: 2.0583e-08 - val_accuracy: 1.0000\n",
            "Epoch 230/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 2.8678e-08 - accuracy: 1.0000 - val_loss: 2.0549e-08 - val_accuracy: 1.0000\n",
            "Epoch 231/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 2.8163e-08 - accuracy: 1.0000 - val_loss: 2.0639e-08 - val_accuracy: 1.0000\n",
            "Epoch 232/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.7639e-08 - accuracy: 1.0000 - val_loss: 1.9634e-08 - val_accuracy: 1.0000\n",
            "Epoch 233/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.6567e-08 - accuracy: 1.0000 - val_loss: 2.0129e-08 - val_accuracy: 1.0000\n",
            "Epoch 234/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.6117e-08 - accuracy: 1.0000 - val_loss: 1.9811e-08 - val_accuracy: 1.0000\n",
            "Epoch 235/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.5297e-08 - accuracy: 1.0000 - val_loss: 2.0661e-08 - val_accuracy: 1.0000\n",
            "Epoch 236/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.5566e-08 - accuracy: 1.0000 - val_loss: 1.8791e-08 - val_accuracy: 1.0000\n",
            "Epoch 237/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 2.4154e-08 - accuracy: 1.0000 - val_loss: 1.8184e-08 - val_accuracy: 1.0000\n",
            "Epoch 238/1000\n",
            "212/212 [==============================] - 2s 12ms/step - loss: 2.3744e-08 - accuracy: 1.0000 - val_loss: 1.8201e-08 - val_accuracy: 1.0000\n",
            "Epoch 239/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.3400e-08 - accuracy: 1.0000 - val_loss: 1.8794e-08 - val_accuracy: 1.0000\n",
            "Epoch 240/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.2854e-08 - accuracy: 1.0000 - val_loss: 1.8186e-08 - val_accuracy: 1.0000\n",
            "Epoch 241/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.2633e-08 - accuracy: 1.0000 - val_loss: 1.7916e-08 - val_accuracy: 1.0000\n",
            "Epoch 242/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.2207e-08 - accuracy: 1.0000 - val_loss: 1.8168e-08 - val_accuracy: 1.0000\n",
            "Epoch 243/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.1927e-08 - accuracy: 1.0000 - val_loss: 1.7841e-08 - val_accuracy: 1.0000\n",
            "Epoch 244/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 2.1456e-08 - accuracy: 1.0000 - val_loss: 1.7632e-08 - val_accuracy: 1.0000\n",
            "Epoch 245/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 2.1088e-08 - accuracy: 1.0000 - val_loss: 1.7131e-08 - val_accuracy: 1.0000\n",
            "Epoch 246/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 2.0667e-08 - accuracy: 1.0000 - val_loss: 1.8527e-08 - val_accuracy: 1.0000\n",
            "Epoch 247/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.1328e-08 - accuracy: 1.0000 - val_loss: 1.7799e-08 - val_accuracy: 1.0000\n",
            "Epoch 248/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 2.0867e-08 - accuracy: 1.0000 - val_loss: 1.8393e-08 - val_accuracy: 1.0000\n",
            "Epoch 249/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 2.0642e-08 - accuracy: 1.0000 - val_loss: 1.8034e-08 - val_accuracy: 1.0000\n",
            "Epoch 250/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.9933e-08 - accuracy: 1.0000 - val_loss: 1.6846e-08 - val_accuracy: 1.0000\n",
            "Epoch 251/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.9832e-08 - accuracy: 1.0000 - val_loss: 1.7331e-08 - val_accuracy: 1.0000\n",
            "Epoch 252/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.9768e-08 - accuracy: 1.0000 - val_loss: 1.7339e-08 - val_accuracy: 1.0000\n",
            "Epoch 253/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.8979e-08 - accuracy: 1.0000 - val_loss: 1.5975e-08 - val_accuracy: 1.0000\n",
            "Epoch 254/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.9258e-08 - accuracy: 1.0000 - val_loss: 1.7272e-08 - val_accuracy: 1.0000\n",
            "Epoch 255/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.9303e-08 - accuracy: 1.0000 - val_loss: 1.6185e-08 - val_accuracy: 1.0000\n",
            "Epoch 256/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.9097e-08 - accuracy: 1.0000 - val_loss: 1.6844e-08 - val_accuracy: 1.0000\n",
            "Epoch 257/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.9135e-08 - accuracy: 1.0000 - val_loss: 1.6321e-08 - val_accuracy: 1.0000\n",
            "Epoch 258/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.9448e-08 - accuracy: 1.0000 - val_loss: 1.6305e-08 - val_accuracy: 1.0000\n",
            "Epoch 259/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.8614e-08 - accuracy: 1.0000 - val_loss: 1.7001e-08 - val_accuracy: 1.0000\n",
            "Epoch 260/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.8334e-08 - accuracy: 1.0000 - val_loss: 1.5782e-08 - val_accuracy: 1.0000\n",
            "Epoch 261/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.8407e-08 - accuracy: 1.0000 - val_loss: 1.7008e-08 - val_accuracy: 1.0000\n",
            "Epoch 262/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.8524e-08 - accuracy: 1.0000 - val_loss: 1.5941e-08 - val_accuracy: 1.0000\n",
            "Epoch 263/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.8835e-08 - accuracy: 1.0000 - val_loss: 1.6953e-08 - val_accuracy: 1.0000\n",
            "Epoch 264/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.9034e-08 - accuracy: 1.0000 - val_loss: 1.5849e-08 - val_accuracy: 1.0000\n",
            "Epoch 265/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7412e-08 - accuracy: 1.0000 - val_loss: 1.5270e-08 - val_accuracy: 1.0000\n",
            "Epoch 266/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.8063e-08 - accuracy: 1.0000 - val_loss: 1.6614e-08 - val_accuracy: 1.0000\n",
            "Epoch 267/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.8400e-08 - accuracy: 1.0000 - val_loss: 1.6504e-08 - val_accuracy: 1.0000\n",
            "Epoch 268/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.7879e-08 - accuracy: 1.0000 - val_loss: 1.5921e-08 - val_accuracy: 1.0000\n",
            "Epoch 269/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7301e-08 - accuracy: 1.0000 - val_loss: 1.4363e-08 - val_accuracy: 1.0000\n",
            "Epoch 270/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6615e-08 - accuracy: 1.0000 - val_loss: 1.4443e-08 - val_accuracy: 1.0000\n",
            "Epoch 271/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7500e-08 - accuracy: 1.0000 - val_loss: 1.4922e-08 - val_accuracy: 1.0000\n",
            "Epoch 272/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7315e-08 - accuracy: 1.0000 - val_loss: 1.5862e-08 - val_accuracy: 1.0000\n",
            "Epoch 273/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.8535e-08 - accuracy: 1.0000 - val_loss: 1.6412e-08 - val_accuracy: 1.0000\n",
            "Epoch 274/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.7841e-08 - accuracy: 1.0000 - val_loss: 1.6037e-08 - val_accuracy: 1.0000\n",
            "Epoch 275/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.7547e-08 - accuracy: 1.0000 - val_loss: 1.5207e-08 - val_accuracy: 1.0000\n",
            "Epoch 276/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.7407e-08 - accuracy: 1.0000 - val_loss: 1.6314e-08 - val_accuracy: 1.0000\n",
            "Epoch 277/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.8373e-08 - accuracy: 1.0000 - val_loss: 1.6262e-08 - val_accuracy: 1.0000\n",
            "Epoch 278/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7753e-08 - accuracy: 1.0000 - val_loss: 1.6705e-08 - val_accuracy: 1.0000\n",
            "Epoch 279/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.7768e-08 - accuracy: 1.0000 - val_loss: 1.5160e-08 - val_accuracy: 1.0000\n",
            "Epoch 280/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6313e-08 - accuracy: 1.0000 - val_loss: 1.5324e-08 - val_accuracy: 1.0000\n",
            "Epoch 281/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6918e-08 - accuracy: 1.0000 - val_loss: 1.5990e-08 - val_accuracy: 1.0000\n",
            "Epoch 282/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.6950e-08 - accuracy: 1.0000 - val_loss: 1.5093e-08 - val_accuracy: 1.0000\n",
            "Epoch 283/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.7609e-08 - accuracy: 1.0000 - val_loss: 1.6696e-08 - val_accuracy: 1.0000\n",
            "Epoch 284/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.7235e-08 - accuracy: 1.0000 - val_loss: 1.5546e-08 - val_accuracy: 1.0000\n",
            "Epoch 285/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6887e-08 - accuracy: 1.0000 - val_loss: 1.5084e-08 - val_accuracy: 1.0000\n",
            "Epoch 286/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6795e-08 - accuracy: 1.0000 - val_loss: 1.5968e-08 - val_accuracy: 1.0000\n",
            "Epoch 287/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7734e-08 - accuracy: 1.0000 - val_loss: 1.6016e-08 - val_accuracy: 1.0000\n",
            "Epoch 288/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.8239e-08 - accuracy: 1.0000 - val_loss: 1.6575e-08 - val_accuracy: 1.0000\n",
            "Epoch 289/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.7646e-08 - accuracy: 1.0000 - val_loss: 1.6000e-08 - val_accuracy: 1.0000\n",
            "Epoch 290/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.7399e-08 - accuracy: 1.0000 - val_loss: 1.4075e-08 - val_accuracy: 1.0000\n",
            "Epoch 291/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6199e-08 - accuracy: 1.0000 - val_loss: 1.5107e-08 - val_accuracy: 1.0000\n",
            "Epoch 292/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6804e-08 - accuracy: 1.0000 - val_loss: 1.4615e-08 - val_accuracy: 1.0000\n",
            "Epoch 293/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6268e-08 - accuracy: 1.0000 - val_loss: 1.4289e-08 - val_accuracy: 1.0000\n",
            "Epoch 294/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.5353e-08 - accuracy: 1.0000 - val_loss: 1.3817e-08 - val_accuracy: 1.0000\n",
            "Epoch 295/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.5483e-08 - accuracy: 1.0000 - val_loss: 1.4403e-08 - val_accuracy: 1.0000\n",
            "Epoch 296/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.5833e-08 - accuracy: 1.0000 - val_loss: 1.4974e-08 - val_accuracy: 1.0000\n",
            "Epoch 297/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.6122e-08 - accuracy: 1.0000 - val_loss: 1.5304e-08 - val_accuracy: 1.0000\n",
            "Epoch 298/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.6733e-08 - accuracy: 1.0000 - val_loss: 1.4805e-08 - val_accuracy: 1.0000\n",
            "Epoch 299/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6512e-08 - accuracy: 1.0000 - val_loss: 1.5665e-08 - val_accuracy: 1.0000\n",
            "Epoch 300/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7799e-08 - accuracy: 1.0000 - val_loss: 1.7074e-08 - val_accuracy: 1.0000\n",
            "Epoch 301/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7208e-08 - accuracy: 1.0000 - val_loss: 1.5409e-08 - val_accuracy: 1.0000\n",
            "Epoch 302/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7225e-08 - accuracy: 1.0000 - val_loss: 1.5888e-08 - val_accuracy: 1.0000\n",
            "Epoch 303/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6426e-08 - accuracy: 1.0000 - val_loss: 1.4634e-08 - val_accuracy: 1.0000\n",
            "Epoch 304/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.6767e-08 - accuracy: 1.0000 - val_loss: 1.5663e-08 - val_accuracy: 1.0000\n",
            "Epoch 305/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.7058e-08 - accuracy: 1.0000 - val_loss: 1.5908e-08 - val_accuracy: 1.0000\n",
            "Epoch 306/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.7364e-08 - accuracy: 1.0000 - val_loss: 1.6106e-08 - val_accuracy: 1.0000\n",
            "Epoch 307/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7167e-08 - accuracy: 1.0000 - val_loss: 1.5751e-08 - val_accuracy: 1.0000\n",
            "Epoch 308/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6698e-08 - accuracy: 1.0000 - val_loss: 1.5149e-08 - val_accuracy: 1.0000\n",
            "Epoch 309/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6226e-08 - accuracy: 1.0000 - val_loss: 1.5336e-08 - val_accuracy: 1.0000\n",
            "Epoch 310/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7019e-08 - accuracy: 1.0000 - val_loss: 1.5497e-08 - val_accuracy: 1.0000\n",
            "Epoch 311/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6663e-08 - accuracy: 1.0000 - val_loss: 1.5451e-08 - val_accuracy: 1.0000\n",
            "Epoch 312/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.7288e-08 - accuracy: 1.0000 - val_loss: 1.6012e-08 - val_accuracy: 1.0000\n",
            "Epoch 313/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.6784e-08 - accuracy: 1.0000 - val_loss: 1.5125e-08 - val_accuracy: 1.0000\n",
            "Epoch 314/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6082e-08 - accuracy: 1.0000 - val_loss: 1.3632e-08 - val_accuracy: 1.0000\n",
            "Epoch 315/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.5262e-08 - accuracy: 1.0000 - val_loss: 1.3831e-08 - val_accuracy: 1.0000\n",
            "Epoch 316/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.5625e-08 - accuracy: 1.0000 - val_loss: 1.4804e-08 - val_accuracy: 1.0000\n",
            "Epoch 317/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6469e-08 - accuracy: 1.0000 - val_loss: 1.5062e-08 - val_accuracy: 1.0000\n",
            "Epoch 318/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6428e-08 - accuracy: 1.0000 - val_loss: 1.5504e-08 - val_accuracy: 1.0000\n",
            "Epoch 319/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.7339e-08 - accuracy: 1.0000 - val_loss: 1.5184e-08 - val_accuracy: 1.0000\n",
            "Epoch 320/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.6286e-08 - accuracy: 1.0000 - val_loss: 1.5083e-08 - val_accuracy: 1.0000\n",
            "Epoch 321/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.6500e-08 - accuracy: 1.0000 - val_loss: 1.5272e-08 - val_accuracy: 1.0000\n",
            "Epoch 322/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.7199e-08 - accuracy: 1.0000 - val_loss: 1.6304e-08 - val_accuracy: 1.0000\n",
            "Epoch 323/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.7293e-08 - accuracy: 1.0000 - val_loss: 1.4953e-08 - val_accuracy: 1.0000\n",
            "Epoch 324/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6404e-08 - accuracy: 1.0000 - val_loss: 1.5096e-08 - val_accuracy: 1.0000\n",
            "Epoch 325/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6795e-08 - accuracy: 1.0000 - val_loss: 1.6149e-08 - val_accuracy: 1.0000\n",
            "Epoch 326/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6977e-08 - accuracy: 1.0000 - val_loss: 1.6186e-08 - val_accuracy: 1.0000\n",
            "Epoch 327/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.5832e-08 - accuracy: 1.0000 - val_loss: 1.5070e-08 - val_accuracy: 1.0000\n",
            "Epoch 328/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.6359e-08 - accuracy: 1.0000 - val_loss: 1.5453e-08 - val_accuracy: 1.0000\n",
            "Epoch 329/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6450e-08 - accuracy: 1.0000 - val_loss: 1.4295e-08 - val_accuracy: 1.0000\n",
            "Epoch 330/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.5377e-08 - accuracy: 1.0000 - val_loss: 1.4454e-08 - val_accuracy: 1.0000\n",
            "Epoch 331/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.5980e-08 - accuracy: 1.0000 - val_loss: 1.5420e-08 - val_accuracy: 1.0000\n",
            "Epoch 332/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6134e-08 - accuracy: 1.0000 - val_loss: 1.5177e-08 - val_accuracy: 1.0000\n",
            "Epoch 333/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.4777e-08 - accuracy: 1.0000 - val_loss: 1.3135e-08 - val_accuracy: 1.0000\n",
            "Epoch 334/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.4319e-08 - accuracy: 1.0000 - val_loss: 1.3973e-08 - val_accuracy: 1.0000\n",
            "Epoch 335/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.5185e-08 - accuracy: 1.0000 - val_loss: 1.4859e-08 - val_accuracy: 1.0000\n",
            "Epoch 336/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.5659e-08 - accuracy: 1.0000 - val_loss: 1.4813e-08 - val_accuracy: 1.0000\n",
            "Epoch 337/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.4667e-08 - accuracy: 1.0000 - val_loss: 1.4198e-08 - val_accuracy: 1.0000\n",
            "Epoch 338/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.5085e-08 - accuracy: 1.0000 - val_loss: 1.4236e-08 - val_accuracy: 1.0000\n",
            "Epoch 339/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.5274e-08 - accuracy: 1.0000 - val_loss: 1.4708e-08 - val_accuracy: 1.0000\n",
            "Epoch 340/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.5616e-08 - accuracy: 1.0000 - val_loss: 1.4920e-08 - val_accuracy: 1.0000\n",
            "Epoch 341/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6093e-08 - accuracy: 1.0000 - val_loss: 1.5729e-08 - val_accuracy: 1.0000\n",
            "Epoch 342/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.6310e-08 - accuracy: 1.0000 - val_loss: 1.5690e-08 - val_accuracy: 1.0000\n",
            "Epoch 343/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.6690e-08 - accuracy: 1.0000 - val_loss: 1.5917e-08 - val_accuracy: 1.0000\n",
            "Epoch 344/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.4801e-08 - accuracy: 1.0000 - val_loss: 1.2170e-08 - val_accuracy: 1.0000\n",
            "Epoch 345/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.2840e-08 - accuracy: 1.0000 - val_loss: 1.1848e-08 - val_accuracy: 1.0000\n",
            "Epoch 346/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.3080e-08 - accuracy: 1.0000 - val_loss: 1.2521e-08 - val_accuracy: 1.0000\n",
            "Epoch 347/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.3783e-08 - accuracy: 1.0000 - val_loss: 1.3234e-08 - val_accuracy: 1.0000\n",
            "Epoch 348/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.3988e-08 - accuracy: 1.0000 - val_loss: 1.3000e-08 - val_accuracy: 1.0000\n",
            "Epoch 349/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.4218e-08 - accuracy: 1.0000 - val_loss: 1.3708e-08 - val_accuracy: 1.0000\n",
            "Epoch 350/1000\n",
            "212/212 [==============================] - 2s 12ms/step - loss: 1.4615e-08 - accuracy: 1.0000 - val_loss: 1.3782e-08 - val_accuracy: 1.0000\n",
            "Epoch 351/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.4986e-08 - accuracy: 1.0000 - val_loss: 1.4389e-08 - val_accuracy: 1.0000\n",
            "Epoch 352/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.5479e-08 - accuracy: 1.0000 - val_loss: 1.4753e-08 - val_accuracy: 1.0000\n",
            "Epoch 353/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.5694e-08 - accuracy: 1.0000 - val_loss: 1.4413e-08 - val_accuracy: 1.0000\n",
            "Epoch 354/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.5629e-08 - accuracy: 1.0000 - val_loss: 1.4952e-08 - val_accuracy: 1.0000\n",
            "Epoch 355/1000\n",
            "212/212 [==============================] - 2s 8ms/step - loss: 1.6165e-08 - accuracy: 1.0000 - val_loss: 1.5053e-08 - val_accuracy: 1.0000\n",
            "Epoch 356/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6654e-08 - accuracy: 1.0000 - val_loss: 1.5429e-08 - val_accuracy: 1.0000\n",
            "Epoch 357/1000\n",
            "212/212 [==============================] - 2s 12ms/step - loss: 1.6562e-08 - accuracy: 1.0000 - val_loss: 1.5280e-08 - val_accuracy: 1.0000\n",
            "Epoch 358/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.6649e-08 - accuracy: 1.0000 - val_loss: 1.6015e-08 - val_accuracy: 1.0000\n",
            "Epoch 359/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.6233e-08 - accuracy: 1.0000 - val_loss: 1.4346e-08 - val_accuracy: 1.0000\n",
            "Epoch 360/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.5800e-08 - accuracy: 1.0000 - val_loss: 1.4861e-08 - val_accuracy: 1.0000\n",
            "Epoch 361/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.5509e-08 - accuracy: 1.0000 - val_loss: 1.4381e-08 - val_accuracy: 1.0000\n",
            "Epoch 362/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.5901e-08 - accuracy: 1.0000 - val_loss: 1.5121e-08 - val_accuracy: 1.0000\n",
            "Epoch 363/1000\n",
            "212/212 [==============================] - 2s 10ms/step - loss: 1.5939e-08 - accuracy: 1.0000 - val_loss: 1.4076e-08 - val_accuracy: 1.0000\n",
            "Epoch 364/1000\n",
            "212/212 [==============================] - 2s 11ms/step - loss: 1.5648e-08 - accuracy: 1.0000 - val_loss: 1.4783e-08 - val_accuracy: 1.0000\n",
            "Epoch 365/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6326e-08 - accuracy: 1.0000 - val_loss: 1.5304e-08 - val_accuracy: 1.0000\n",
            "Epoch 366/1000\n",
            "212/212 [==============================] - 2s 9ms/step - loss: 1.6606e-08 - accuracy: 1.0000 - val_loss: 1.4833e-08 - val_accuracy: 1.0000\n",
            "Epoch 367/1000\n",
            " 24/212 [==>...........................] - ETA: 1s - loss: 1.7705e-08 - accuracy: 1.0000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c5ETf4-OjGSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conv1D model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "X_train_resampled = np.expand_dims(X_train_resampled, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "def create_advanced_model():\n",
        "    input_layer = keras.Input(shape=(X_train_resampled.shape[1], X_train_resampled.shape[2]))\n",
        "\n",
        "    x = layers.Conv1D(\n",
        "        filters=64, kernel_size=5, strides=2, padding=\"same\"\n",
        "    )(input_layer)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Add multiple residual blocks with varying depths and filter sizes\n",
        "    for _ in range(4):\n",
        "        x_res = x\n",
        "\n",
        "        x = layers.Conv1D(\n",
        "            filters=128, kernel_size=3, strides=1, padding=\"same\"\n",
        "        )(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.Conv1D(\n",
        "            filters=128, kernel_size=3, strides=1, padding=\"same\"\n",
        "        )(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        # Adjust the number of filters in x_res to match x\n",
        "        if x_res.shape[-1] != x.shape[-1]:\n",
        "            x_res = layers.Conv1D(filters=x.shape[-1], kernel_size=1, strides=1, padding=\"same\")(x_res)\n",
        "\n",
        "        x = layers.add([x, x_res])  # Residual connection\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # Global Average Pooling\n",
        "\n",
        "    x = layers.Dense(2048, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    x = layers.Dense(1024, activation=\"relu\", kernel_regularizer=keras.regularizers.L2())(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.L2())(x)\n",
        "\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model = create_advanced_model()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/best_model_2_gpt_deneme.h5', save_best_only=True)\n",
        "\n",
        "model.fit(X_train_resampled, y_train_resampled, epochs=1000, batch_size=64, validation_split=0.2,\n",
        "          callbacks=[early_stopping, model_checkpoint], class_weight=class_weight)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "J1wt4y6SAmBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save(\"xu030-long-deneme.h5\")\n",
        "model = tf.keras.models.load_model(\"/content/lstm_model.h5\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "jqRgrzZg084k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for stock in [\"EXPE\"]:\n",
        "  predictions = []\n",
        "\n",
        "  stock_data = yf.download(stock, start=\"2023-01-01\", end=\"2023-08-10\", progress=False)\n",
        "  stock_data = create_indicators(stock_data)\n",
        "\n",
        "  stock_data[\"signal\"] = 0\n",
        "\n",
        "  for row in range(stock_data.shape[0]):\n",
        "    if row+1 != stock_data.shape[0] and stock_data[\"Close\"].iloc[row+1] > stock_data[\"Close\"].iloc[row] + 2*(stock_data[\"Close\"].iloc[row])/100:\n",
        "      stock_data[\"signal\"].iloc[row] = 1\n",
        "\n",
        "  stock_data = stock_data.drop([\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"], axis=1)\n",
        "\n",
        "  for index, row in stock_data.iterrows():\n",
        "      x = row[1:data.shape[1]]\n",
        "\n",
        "      new_data = x.to_numpy().reshape(1, -1)\n",
        "      new_data = scaler.transform(new_data)\n",
        "      prediction = model.predict(new_data, verbose=None)\n",
        "\n",
        "      print(f\"Prediction for date {row[0]} {row[-1]}: {np.round(prediction[0][0])}\")\n",
        "\n",
        "      if np.round(prediction[0][0]) == row[\"signal\"]:\n",
        "        predictions.append(1)\n",
        "      elif np.round(prediction[0][0]) != row[\"signal\"]:\n",
        "        predictions.append(0)\n",
        "\n",
        "  print(f\"{stock} Accuracy: {predictions.count(1) / len(predictions) * 100}\")"
      ],
      "metadata": {
        "id": "GXskMvribBZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bc16b9-184e-4fa9-d0b8-a2755ce7a266"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for date 2023-03-30 00:00:00 0: 0.0\n",
            "Prediction for date 2023-03-31 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-03 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-04 00:00:00 0: 1.0\n",
            "Prediction for date 2023-04-05 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-06 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-10 00:00:00 1: 1.0\n",
            "Prediction for date 2023-04-11 00:00:00 0: 1.0\n",
            "Prediction for date 2023-04-12 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-13 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-14 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-17 00:00:00 1: 0.0\n",
            "Prediction for date 2023-04-18 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-19 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-20 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-21 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-24 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-25 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-26 00:00:00 1: 0.0\n",
            "Prediction for date 2023-04-27 00:00:00 0: 0.0\n",
            "Prediction for date 2023-04-28 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-01 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-02 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-03 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-04 00:00:00 1: 0.0\n",
            "Prediction for date 2023-05-05 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-08 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-09 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-10 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-11 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-12 00:00:00 1: 1.0\n",
            "Prediction for date 2023-05-15 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-16 00:00:00 1: 1.0\n",
            "Prediction for date 2023-05-17 00:00:00 1: 0.0\n",
            "Prediction for date 2023-05-18 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-19 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-22 00:00:00 0: 1.0\n",
            "Prediction for date 2023-05-23 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-24 00:00:00 1: 0.0\n",
            "Prediction for date 2023-05-25 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-26 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-30 00:00:00 0: 0.0\n",
            "Prediction for date 2023-05-31 00:00:00 1: 0.0\n",
            "Prediction for date 2023-06-01 00:00:00 1: 0.0\n",
            "Prediction for date 2023-06-02 00:00:00 1: 0.0\n",
            "Prediction for date 2023-06-05 00:00:00 1: 0.0\n",
            "Prediction for date 2023-06-06 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-07 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-08 00:00:00 1: 0.0\n",
            "Prediction for date 2023-06-09 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-12 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-13 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-14 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-15 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-16 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-20 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-21 00:00:00 1: 0.0\n",
            "Prediction for date 2023-06-22 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-23 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-26 00:00:00 1: 0.0\n",
            "Prediction for date 2023-06-27 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-28 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-29 00:00:00 0: 0.0\n",
            "Prediction for date 2023-06-30 00:00:00 1: 0.0\n",
            "Prediction for date 2023-07-03 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-05 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-06 00:00:00 1: 0.0\n",
            "Prediction for date 2023-07-07 00:00:00 1: 0.0\n",
            "Prediction for date 2023-07-10 00:00:00 1: 0.0\n",
            "Prediction for date 2023-07-11 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-12 00:00:00 1: 0.0\n",
            "Prediction for date 2023-07-13 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-14 00:00:00 1: 0.0\n",
            "Prediction for date 2023-07-17 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-18 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-19 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-20 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-21 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-24 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-25 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-26 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-27 00:00:00 0: 0.0\n",
            "Prediction for date 2023-07-28 00:00:00 1: 0.0\n",
            "Prediction for date 2023-07-31 00:00:00 0: 0.0\n",
            "Prediction for date 2023-08-01 00:00:00 0: 0.0\n",
            "Prediction for date 2023-08-02 00:00:00 0: 0.0\n",
            "Prediction for date 2023-08-03 00:00:00 1: 1.0\n",
            "Prediction for date 2023-08-04 00:00:00 1: 1.0\n",
            "Prediction for date 2023-08-07 00:00:00 1: 0.0\n",
            "Prediction for date 2023-08-08 00:00:00 0: 1.0\n",
            "Prediction for date 2023-08-09 00:00:00 0: 0.0\n",
            "EXPE Accuracy: 73.62637362637363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buy_stocks = []\n",
        "\n",
        "for stock in result_df[\"STOCK\"].unique():\n",
        "  stock_data = yf.download(stock, start=\"2021-06-01\", end=\"2023-08-17\", progress=False)\n",
        "  stock_data = create_indicators(stock_data)\n",
        "\n",
        "  change = ((stock_data[\"Close\"].iloc[-1] - stock_data[\"Close\"].iloc[-2]) / stock_data[\"Close\"].iloc[-2])*100\n",
        "  change = round(change, 2)\n",
        "\n",
        "  stock_data = stock_data.drop([\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"], axis=1)\n",
        "\n",
        "  x = stock_data.iloc[-2]\n",
        "\n",
        "  new_data = x.to_numpy().reshape(1, -1)\n",
        "  new_data = scaler.transform(new_data)\n",
        "  prediction = model.predict(new_data, verbose=None)\n",
        "\n",
        "  if round(prediction[0][0]*100,2) > 75:\n",
        "    buy_stocks.append([stock, round(prediction[0][0]*100,2), change])\n",
        "\n",
        "buy_df = pd.DataFrame(buy_stocks, columns=[\"stock\", \"probability\", \"change\"]).sort_values(by=\"probability\", ascending=False)\n",
        "print(f'Pozitif kapanan hisse says: {buy_df[buy_df[\"change\"] > 0].shape[0]}')\n",
        "print(f'Negatif kapanan hisse says: {buy_df[buy_df[\"change\"] < 0].shape[0]}')\n",
        "print(f'Gnlk deiim ortalamas: %{round(buy_df[\"change\"].sum()/buy_df.shape[0], 2)}\\n')\n",
        "buy_df"
      ],
      "metadata": {
        "id": "YASh8fggUiZk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "230139f7-e43c-494d-e2be-d481cda0427c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pozitif kapanan hisse says: 2\n",
            "Negatif kapanan hisse says: 3\n",
            "Gnlk deiim ortalamas: %-0.94\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      stock  probability  change\n",
              "3  PGSUS.IS       100.00   -1.96\n",
              "1  GARAN.IS        99.71   -3.15\n",
              "4  THYAO.IS        96.80   -2.00\n",
              "0  ENKAI.IS        89.29    1.82\n",
              "2  KOZAL.IS        77.66    0.60"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6893b130-827f-4692-a16c-48046066fbd6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock</th>\n",
              "      <th>probability</th>\n",
              "      <th>change</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PGSUS.IS</td>\n",
              "      <td>100.00</td>\n",
              "      <td>-1.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GARAN.IS</td>\n",
              "      <td>99.71</td>\n",
              "      <td>-3.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>THYAO.IS</td>\n",
              "      <td>96.80</td>\n",
              "      <td>-2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENKAI.IS</td>\n",
              "      <td>89.29</td>\n",
              "      <td>1.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KOZAL.IS</td>\n",
              "      <td>77.66</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6893b130-827f-4692-a16c-48046066fbd6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6893b130-827f-4692-a16c-48046066fbd6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6893b130-827f-4692-a16c-48046066fbd6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e9a8ac35-5521-4880-8551-fe82e57eee5d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e9a8ac35-5521-4880-8551-fe82e57eee5d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e9a8ac35-5521-4880-8551-fe82e57eee5d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2023-08-08\n",
        "1\tBIMAS.IS\t98.39\t-1.34\n",
        "4\tPGSUS.IS\t97.33\t9.52\n",
        "2\tGARAN.IS\t96.60\t-0.43\n",
        "5\tPETKM.IS\t90.65\t5.20\n",
        "3\tGUBRF.IS\t90.02\t-0.40\n",
        "0\tARCLK.IS\t83.10\t1.14\n",
        "\n",
        "2023-08-08\n",
        "2\tPGSUS.IS\t100.00\t0.33\n",
        "1\tGARAN.IS\t97.05\t3.03\n",
        "0\tBIMAS.IS\t96.98\t1.54\n",
        "3\tSASA.IS\t95.37\t-3.57\n",
        "4\tTCELL.IS\t93.31\t0.94\n",
        "\n",
        "2023-08-09\n",
        "0\tBIMAS.IS\t99.82\t9.99\n",
        "2\tPETKM.IS\t99.82\t0.33\n",
        "3\tTHYAO.IS\t99.22\t-0.69\n",
        "1\tGARAN.IS\t92.52\t9.91\n",
        "\n",
        "2023-08-10\n",
        "2\tTOASO.IS\t98.16\t-4.08\n",
        "0\tGARAN.IS\t85.31\t2.83\n",
        "1\tSAHOL.IS\t76.00\t-0.84\n",
        "\n",
        "2023-08-11\n",
        "1\tKRDMD.IS\t100.00\t2.44\n",
        "2\tTHYAO.IS\t97.89\t2.79\n",
        "0\tFROTO.IS\t90.15\t2.24\n",
        "\n",
        "2023-08-14\n",
        "3\tTHYAO.IS\t99.93\t-1.62\n",
        "0\tAKBNK.IS\t99.74\t-2.20\n",
        "1\tGARAN.IS\t96.38\t0.63\n",
        "2\tTAVHL.IS\t87.66\t5.62\n",
        "\n",
        "2023-08-15\n",
        "3\tTHYAO.IS\t100.00\t-0.65\n",
        "0\tAKBNK.IS\t99.94\t0.53\n",
        "2\tSASA.IS\t99.93\t-0.99\n",
        "1\tGARAN.IS\t97.06\t-0.63\n",
        "\n",
        "2023-08-16\n",
        "1\tPGSUS.IS\t99.99\t-1.96\n",
        "0\tGARAN.IS\t99.91\t-3.15\n",
        "2\tSASA.IS\t99.26\t-1.28\n",
        "3\tPETKM.IS\t76.96\t0.98\n",
        "\n",
        "2023-08-17\n",
        "1\tPGSUS.IS\t100.00\t4.82\n",
        "5\tTOASO.IS\t100.00\t2.20\n",
        "0\tODAS.IS\t99.83\t-1.25\n",
        "6\tISCTR.IS\t97.81\t0.94\n",
        "7\tYKBNK.IS\t94.71\t2.77\n",
        "3\tSASA.IS\t84.09\t0.00\n",
        "4\tTAVHL.IS\t81.61\t0.39\n",
        "2\tSAHOL.IS\t79.27\t0.74\n",
        "\n",
        "2023-08-18\n",
        "0\tEKGYO.IS\t99.99\t-4.26\n",
        "3\tTOASO.IS\t99.95\t-3.51\n",
        "1\tGARAN.IS\t99.64\t-6.55\n",
        "2\tPGSUS.IS\t99.56\t-4.95\n",
        "4\tISCTR.IS\t96.21\t-3.49"
      ],
      "metadata": {
        "id": "3Ccc0G_6rn1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZOJm4USTjy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
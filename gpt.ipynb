{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erendagasan/Eren-Dagasan-Personal/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Libraries and Indicator Function\n",
        "\n",
        "!pip install -q bta-lib\n",
        "!pip install -q ta\n",
        "\n",
        "import btalib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ta.trend import PSARIndicator\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from ta.trend import AroonIndicator\n",
        "from ta.volume import VolumePriceTrendIndicator\n",
        "from ta.trend import CCIIndicator\n",
        "from ta.momentum import ROCIndicator\n",
        "from ta.trend import ADXIndicator\n",
        "from ta.momentum import ultimate_oscillator\n",
        "from ta.volume import ChaikinMoneyFlowIndicator\n",
        "from ta.trend import KSTIndicator\n",
        "from ta.momentum import TSIIndicator\n",
        "from ta.trend import WMAIndicator\n",
        "import yfinance as yf\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def create_indicators(data):\n",
        "  data[\"RSI-14\"] = btalib.rsi(data[\"Close\"], period=14).df\n",
        "  data[\"RSI-60\"] = btalib.rsi(data[\"Close\"], period=60).df\n",
        "\n",
        "  # data[\"STOCH-K\"] = btalib.stoch(data['High'], data['Low'], data['Close']).df[\"k\"]\n",
        "  # data[\"STOCH-D\"] = btalib.stoch(data['High'], data['Low'], data['Close']).df[\"d\"]\n",
        "\n",
        "  # data[\"WILLIAMS\"] = WilliamsRIndicator(data[\"High\"], data[\"Low\"], data[\"Close\"]).williams_r()\n",
        "  # data[\"AROON\"] = AroonIndicator(close=data[\"Close\"], window=25).aroon_indicator()\n",
        "  # data['CCI'] = CCIIndicator(close=data['Close'], low=data[\"Low\"], high=data[\"High\"], window=14).cci()\n",
        "  # data['ROC'] = ROCIndicator(close=data['Close'], window=5).roc()\n",
        "\n",
        "  # adx_indicator = ADXIndicator(high=data['High'], low=data['Low'], close=data['Close'], window=14)\n",
        "  # data['ADX'] = adx_indicator.adx()\n",
        "  # data['+DI'] = adx_indicator.adx_pos()\n",
        "  # data['-DI'] = adx_indicator.adx_neg()\n",
        "\n",
        "  # data['ULTIMATE-OSC'] = ultimate_oscillator(high=data['High'], low=data['Low'], close=data['Close'], window1=7, window2=14, window3=28)\n",
        "  # data['MONEY-FLOW'] = ChaikinMoneyFlowIndicator(high=data['High'], low=data['Low'], close=data['Close'], volume=data['Volume'], window=20).chaikin_money_flow()\n",
        "  # data['KST'] = KSTIndicator(data['Close']).kst()\n",
        "\n",
        "  # data['TSI'] = TSIIndicator(data['Close']).tsi()\n",
        "\n",
        "  data['WMA-30'] = WMAIndicator(data['Close'], window=30).wma()\n",
        "\n",
        "  data = data.dropna()\n",
        "  data = data.reset_index()\n",
        "  return data\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Lfv1WZ9SIIPB"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the Model\n",
        "\n",
        "import gdown\n",
        "\n",
        "gdown.download(\"https://drive.google.com/u/1/uc?id=1-3r7tu0ZQXWNtqMQ35rX0DexxNt9P1l5&export=download\", \"/content/\", quiet=False)\n",
        "# gdown.download(\"https://drive.google.com/u/0/uc?id=117pezAA6jRLCwIsdpEhZgEa9tEanlC0O&export=download\", \"/content/\", quiet=False)\n",
        "\n",
        "# data = pd.read_csv(\"data.csv\")\n",
        "model = tf.keras.models.load_model(\"best_model_2.h5\")"
      ],
      "metadata": {
        "id": "j0_ac_MyLUax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stock List\n",
        "sheet_id = \"1RSqOXkFTAO7g4H9LEY3d3IX6H6bJaYk1\"\n",
        "sheet_name = \"Sheet_1\"\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "result_df = pd.read_csv(url)\n",
        "\n",
        "sheet_id = \"1AA9MfqOtAAgO97__aomD79DciyT-PkRQ\"\n",
        "sheet_name = \"Sheet_1\"\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "result_df = pd.read_csv(url)\n",
        "\n",
        "nasdaq100 = ['AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN',\n",
        "             'NVDA', 'TSLA', 'META', 'AVGO', 'ASML',\n",
        "             'PEP', 'COST', 'ADBE', 'AZN', 'CSCO',\n",
        "             'NFLX', 'AMD', 'CMCSA', 'TMUS', 'TXN',\n",
        "             'QCOM', 'HON', 'INTU', 'INTC', 'SNY',\n",
        "             'VZ', 'AMGN', 'SBUX', 'ISRG', 'AMAT',\n",
        "             'BKNG', 'ADI', 'MDLZ', 'PDD', 'GILD',\n",
        "             'ADP', 'VRTX', 'ABNB', 'LRCX', 'PYPL',\n",
        "             'REGN', 'EQIX', 'MU', 'CSX', 'SNPS',\n",
        "             'CME', 'CDNS', 'KLAC', 'NTES']"
      ],
      "metadata": {
        "id": "3g0KHl4YMTKd",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "for stock in [\"EXPE\"]:\n",
        "  stock_df = yf.download(stock, start=\"2010-01-01\", end=\"2023-01-01\", progress=False)\n",
        "  stock_df = create_indicators(stock_df)\n",
        "  stock_df[\"signal\"] = 0\n",
        "\n",
        "  for index, row in stock_df.iterrows():\n",
        "    if index > 0 and index < stock_df.shape[0]-1 and stock_df[\"Close\"].iloc[index+1] > ((1*stock_df[\"Close\"].iloc[index]/100) + stock_df[\"Close\"].iloc[index]):\n",
        "      stock_df[\"signal\"].iloc[index] = 1\n",
        "\n",
        "  stock_df = stock_df.drop([\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"], axis=1)\n",
        "\n",
        "  data = pd.concat([data, stock_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "301D-_cbzUbS"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Preprocessing\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "input_columns = df.columns[:data.shape[1]-1]\n",
        "output_column = \"signal\"\n",
        "\n",
        "df[output_column] = df[output_column].astype(int)\n",
        "\n",
        "X = df[input_columns].values\n",
        "y = df[output_column].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(\"balanced\", classes=[0, 1], y=y_train)\n",
        "class_weight = {cls: weight for cls, weight in zip([0, 1], class_weights)}\n",
        "class_weight"
      ],
      "metadata": {
        "id": "K-Gz1pgOMh8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99016ba0-0322-4cfb-ad30-7ce17b9d6969"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.6913347685683531, 1: 1.8066104078762306}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',  # Negative MSE for optimization\n",
        "    cv=5  # Cross-validation folds\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Get the best parameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the Random Forest model using the best parameters\n",
        "model = RandomForestRegressor(**best_params, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "nWnDvwwnvfra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LSTM Model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X_train_resampled.shape[1], 1)),\n",
        "\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=500, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('lstm_model.h5', save_best_only=True)\n",
        "\n",
        "model.fit(X_train_resampled, y_train_resampled, epochs=1000, batch_size=16, validation_split=0.3, class_weight=class_weight,\n",
        "          callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "IfSHE91eZonR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c5ETf4-OjGSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conv1D model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "X_train_resampled = np.expand_dims(X_train_resampled, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "def create_model():\n",
        "    input_layer = keras.Input(shape=(X_train_resampled.shape[1], X_train_resampled.shape[2]))\n",
        "\n",
        "    x = layers.Conv1D(\n",
        "        filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n",
        "    )(input_layer)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv1D(\n",
        "        filters=64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv1D(\n",
        "        filters=128, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv1D(\n",
        "        filters=256, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv1D(\n",
        "        filters=512, kernel_size=7, strides=2, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv1D(\n",
        "        filters=1024, kernel_size=7, strides=2, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    x = layers.Dense(4096, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Dense(\n",
        "        2048, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n",
        "    )(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Dense(\n",
        "        1024, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n",
        "    )(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(\n",
        "        128, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n",
        "    )(x)\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/best_model_2_gpt_deneme.h5', save_best_only=True)\n",
        "\n",
        "model.fit(X_train_resampled, y_train_resampled, epochs=1000, batch_size=64, validation_split=0.2,\n",
        "          callbacks=[early_stopping, model_checkpoint], class_weight=class_weight)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "J1wt4y6SAmBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save(\"xu030-long-deneme.h5\")\n",
        "model = tf.keras.models.load_model(\"/content/lstm_model.h5\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "jqRgrzZg084k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5575f69b-3583-404e-9884-24e29dbbf726"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 1s 4ms/step - loss: 0.7151 - accuracy: 0.4883\n",
            "Test Loss: 0.7151261568069458\n",
            "Test Accuracy: 0.4883359372615814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for stock in [\"EXPE\"]:\n",
        "  predictions = []\n",
        "\n",
        "  stock_data = yf.download(stock, start=\"2023-01-01\", end=\"2023-08-10\", progress=False)\n",
        "  stock_data = create_indicators(stock_data)\n",
        "\n",
        "  stock_data[\"signal\"] = 0\n",
        "\n",
        "  for row in range(stock_data.shape[0]):\n",
        "    if row+1 != stock_data.shape[0] and stock_data[\"Close\"].iloc[row+1] > stock_data[\"Close\"].iloc[row]:\n",
        "      stock_data[\"signal\"].iloc[row] = 1\n",
        "\n",
        "  stock_data = stock_data.drop([\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"], axis=1)\n",
        "\n",
        "  for index, row in stock_data.iterrows():\n",
        "      x = row[1:data.shape[1]]\n",
        "\n",
        "      new_data = x.to_numpy().reshape(1, -1)\n",
        "      new_data = scaler.transform(new_data)\n",
        "      prediction = model.predict(new_data)\n",
        "\n",
        "      print(f\"Prediction for date {row[0]} {row[-1]}: {np.round(prediction)}\")\n",
        "\n",
        "      if np.round(prediction) == row[\"signal\"]:\n",
        "        predictions.append(1)\n",
        "      elif np.round(prediction) != row[\"signal\"]:\n",
        "        predictions.append(0)\n",
        "\n",
        "  print(f\"{stock} Accuracy: {predictions.count(1) / len(predictions) * 100}\")"
      ],
      "metadata": {
        "id": "GXskMvribBZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d6d466-d470-4809-cdaa-892d041e786f"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for date 2023-03-30 00:00:00 1: [0.]\n",
            "Prediction for date 2023-03-31 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-03 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-04 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-05 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-06 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-10 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-11 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-12 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-13 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-14 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-17 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-18 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-19 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-20 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-21 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-24 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-25 00:00:00 0: [0.]\n",
            "Prediction for date 2023-04-26 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-27 00:00:00 1: [0.]\n",
            "Prediction for date 2023-04-28 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-01 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-02 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-03 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-04 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-05 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-08 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-09 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-10 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-11 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-12 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-15 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-16 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-17 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-18 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-19 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-22 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-23 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-24 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-25 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-26 00:00:00 1: [0.]\n",
            "Prediction for date 2023-05-30 00:00:00 0: [0.]\n",
            "Prediction for date 2023-05-31 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-01 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-02 00:00:00 0: [0.]\n",
            "Prediction for date 2023-06-05 00:00:00 0: [0.]\n",
            "Prediction for date 2023-06-06 00:00:00 0: [0.]\n",
            "Prediction for date 2023-06-07 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-08 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-09 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-12 00:00:00 0: [0.]\n",
            "Prediction for date 2023-06-13 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-14 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-15 00:00:00 0: [0.]\n",
            "Prediction for date 2023-06-16 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-20 00:00:00 0: [0.]\n",
            "Prediction for date 2023-06-21 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-22 00:00:00 0: [0.]\n",
            "Prediction for date 2023-06-23 00:00:00 0: [0.]\n",
            "Prediction for date 2023-06-26 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-27 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-28 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-29 00:00:00 1: [0.]\n",
            "Prediction for date 2023-06-30 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-03 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-05 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-06 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-07 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-10 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-11 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-12 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-13 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-14 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-17 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-18 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-19 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-20 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-21 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-24 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-25 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-26 00:00:00 0: [0.]\n",
            "Prediction for date 2023-07-27 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-28 00:00:00 1: [0.]\n",
            "Prediction for date 2023-07-31 00:00:00 0: [0.]\n",
            "Prediction for date 2023-08-01 00:00:00 0: [0.]\n",
            "Prediction for date 2023-08-02 00:00:00 0: [0.]\n",
            "Prediction for date 2023-08-03 00:00:00 0: [0.]\n",
            "Prediction for date 2023-08-04 00:00:00 0: [0.]\n",
            "Prediction for date 2023-08-07 00:00:00 1: [0.]\n",
            "Prediction for date 2023-08-08 00:00:00 0: [0.]\n",
            "Prediction for date 2023-08-09 00:00:00 0: [0.]\n",
            "AAPL Accuracy: 50.54945054945055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buy_stocks = []\n",
        "\n",
        "for stock in result_df[\"STOCK\"].unique():\n",
        "  stock_data = yf.download(stock, start=\"2021-06-01\", end=\"2023-08-17\", progress=False)\n",
        "  stock_data = create_indicators(stock_data)\n",
        "\n",
        "  change = ((stock_data[\"Close\"].iloc[-1] - stock_data[\"Close\"].iloc[-2]) / stock_data[\"Close\"].iloc[-2])*100\n",
        "  change = round(change, 2)\n",
        "\n",
        "  stock_data = stock_data.drop([\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"], axis=1)\n",
        "\n",
        "  x = stock_data.iloc[-2]\n",
        "\n",
        "  new_data = x.to_numpy().reshape(1, -1)\n",
        "  new_data = scaler.transform(new_data)\n",
        "  prediction = model.predict(new_data, verbose=None)\n",
        "\n",
        "  if round(prediction[0][0]*100,2) > 75:\n",
        "    buy_stocks.append([stock, round(prediction[0][0]*100,2), change])\n",
        "\n",
        "buy_df = pd.DataFrame(buy_stocks, columns=[\"stock\", \"probability\", \"change\"]).sort_values(by=\"probability\", ascending=False)\n",
        "print(f'Pozitif kapanan hisse sayısı: {buy_df[buy_df[\"change\"] > 0].shape[0]}')\n",
        "print(f'Negatif kapanan hisse sayısı: {buy_df[buy_df[\"change\"] < 0].shape[0]}')\n",
        "print(f'Günlük değişim ortalaması: %{round(buy_df[\"change\"].sum()/buy_df.shape[0], 2)}\\n')\n",
        "buy_df"
      ],
      "metadata": {
        "id": "YASh8fggUiZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2023-08-08\n",
        "1\tBIMAS.IS\t98.39\t-1.34\n",
        "4\tPGSUS.IS\t97.33\t9.52\n",
        "2\tGARAN.IS\t96.60\t-0.43\n",
        "5\tPETKM.IS\t90.65\t5.20\n",
        "3\tGUBRF.IS\t90.02\t-0.40\n",
        "0\tARCLK.IS\t83.10\t1.14\n",
        "\n",
        "2023-08-08\n",
        "2\tPGSUS.IS\t100.00\t0.33\n",
        "1\tGARAN.IS\t97.05\t3.03\n",
        "0\tBIMAS.IS\t96.98\t1.54\n",
        "3\tSASA.IS\t95.37\t-3.57\n",
        "4\tTCELL.IS\t93.31\t0.94\n",
        "\n",
        "2023-08-09\n",
        "0\tBIMAS.IS\t99.82\t9.99\n",
        "2\tPETKM.IS\t99.82\t0.33\n",
        "3\tTHYAO.IS\t99.22\t-0.69\n",
        "1\tGARAN.IS\t92.52\t9.91\n",
        "\n",
        "2023-08-10\n",
        "2\tTOASO.IS\t98.16\t-4.08\n",
        "0\tGARAN.IS\t85.31\t2.83\n",
        "1\tSAHOL.IS\t76.00\t-0.84\n",
        "\n",
        "2023-08-11\n",
        "1\tKRDMD.IS\t100.00\t2.44\n",
        "2\tTHYAO.IS\t97.89\t2.79\n",
        "0\tFROTO.IS\t90.15\t2.24\n",
        "\n",
        "2023-08-14\n",
        "3\tTHYAO.IS\t99.93\t-1.62\n",
        "0\tAKBNK.IS\t99.74\t-2.20\n",
        "1\tGARAN.IS\t96.38\t0.63\n",
        "2\tTAVHL.IS\t87.66\t5.62\n",
        "\n",
        "2023-08-15\n",
        "3\tTHYAO.IS\t100.00\t-0.65\n",
        "0\tAKBNK.IS\t99.94\t0.53\n",
        "2\tSASA.IS\t99.93\t-0.99\n",
        "1\tGARAN.IS\t97.06\t-0.63\n",
        "\n",
        "2023-08-16\n",
        "1\tPGSUS.IS\t99.99\t-1.96\n",
        "0\tGARAN.IS\t99.91\t-3.15\n",
        "2\tSASA.IS\t99.26\t-1.28\n",
        "3\tPETKM.IS\t76.96\t0.98\n",
        "\n",
        "2023-08-17\n",
        "1\tPGSUS.IS\t100.00\t4.82\n",
        "5\tTOASO.IS\t100.00\t2.20\n",
        "0\tODAS.IS\t99.83\t-1.25\n",
        "6\tISCTR.IS\t97.81\t0.94\n",
        "7\tYKBNK.IS\t94.71\t2.77\n",
        "3\tSASA.IS\t84.09\t0.00\n",
        "4\tTAVHL.IS\t81.61\t0.39\n",
        "2\tSAHOL.IS\t79.27\t0.74\n",
        "\n",
        "2023-08-18\n",
        "0\tEKGYO.IS\t99.99\t-4.26\n",
        "3\tTOASO.IS\t99.95\t-3.51\n",
        "1\tGARAN.IS\t99.64\t-6.55\n",
        "2\tPGSUS.IS\t99.56\t-4.95\n",
        "4\tISCTR.IS\t96.21\t-3.49"
      ],
      "metadata": {
        "id": "3Ccc0G_6rn1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZOJm4USTjy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}